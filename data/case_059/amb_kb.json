{
  "ambiguities": [
    {
      "id": "1_O1_weekend_end_roll_time",
      "kind": "Operation boundary",
      "node_id": "compute_business_days",
      "op": "project",
      "source_text": "if a case is closed on the weekend then we roll this back to the Friday.",
      "ref": "closed_with_end_date.loc[is_saturday_close, 'End Date'] = closed_with_end_date.loc[is_saturday_close, 'End Date'].dt.normalize() - pd.Timedelta(seconds=1)\nclosed_with_end_date.loc[is_sunday_close, 'End Date'] = closed_with_end_date.loc[is_sunday_close, 'End Date'].dt.normalize() - pd.Timedelta(days=1, seconds=1)"
    },
    {
      "id": "2_O1_minute_level_business_days_formula",
      "kind": "Row-level concept",
      "node_id": "compute_business_days",
      "op": "project",
      "source_text": "Business days are also calculated down to the minute level.",
      "ref": "bus_days = np.busday_count(start.date(), end.date())\nstart_time_fraction = (pd.Timedelta(days=1) - (start - start.normalize())) / pd.Timedelta(days=1)\nend_time_fraction = (end - end.normalize()) / pd.Timedelta(days=1)\n\nif bus_days == 0: # Same day\n    return (end - start) / pd.Timedelta(days=1)\n\nbusiness_duration = (bus_days - 1) + start_time_fraction + end_time_fraction\nreturn business_duration"
    },
    {
      "id": "3_O1_weekend_start_roll_time",
      "kind": "Operation boundary",
      "node_id": "compute_business_days",
      "op": "project",
      "source_text": "If a case is logged on the weekend, we roll these to the Monday.",
      "ref": "df.loc[is_saturday, date_col] = df.loc[is_saturday, date_col].dt.normalize() + pd.Timedelta(days=2)\ndf.loc[is_sunday, date_col] = df.loc[is_sunday, date_col].dt.normalize() + pd.Timedelta(days=1)"
    },
    {
      "id": "4_O1_sla_met_inclusive_leq",
      "kind": "Operation boundary",
      "node_id": "compute_met_sla",
      "op": "project",
      "source_text": "the more cases have been closed under the SLA limit set",
      "ref": "closed_analysis['Met SLA'] = closed_analysis['Business Days Open'] <= closed_analysis['SLA Agreement']"
    },
    {
      "id": "5_O3_current_status_by_max_status_no",
      "kind": "Operation inconsistent",
      "node_id": "current_status",
      "op": "dedup",
      "source_text": "This creates a sequential Status No, giving us a history of the progress each ticket has made.",
      "ref": "current_status = tickets.loc[tickets.groupby('Ticket ID')['Status No.'].idxmax()]"
    },
    {
      "id": "6_O3_end_date_latest_closed_event",
      "kind": "Operation inconsistent",
      "node_id": "end_dates",
      "op": "aggregate",
      "intent": "For closed tickets, use the latest Closed status-change timestamp as the ticket end time (when multiple Closed events exist).",
      "ref": "closed_tickets = tickets[tickets['Status Name'] == 'Closed']\nend_dates = closed_tickets.groupby('Ticket ID')['Timestamp'].max().reset_index()"
    },
    {
      "id": "7_O2_exclude_zero_achieved_departments",
      "kind": "Operation boundary",
      "node_id": "filter_achieved",
      "op": "filter",
      "intent": "Exclude departments whose SLA Achieved % is 0 from the department ranking output.",
      "ref": "dept_sla = dept_sla[dept_sla['Achieved %'] > 0]"
    },
    {
      "id": "8_O3_department_attribution_from_current_status",
      "kind": "Operation inconsistent",
      "node_id": "join_department",
      "op": "join",
      "intent": "Attribute each ticket to the Department found on the current-status record for downstream SLA/department analyses.",
      "ref": "ticket_info = current_status[['Ticket ID', 'Department']].drop_duplicates()\nanalysis_df = pd.merge(ticket_durations, ticket_info, on='Ticket ID')"
    },
    {
      "id": "9_D2_join_sla_on_department",
      "kind": "Multi-table alignment",
      "node_id": "join_sla",
      "op": "join",
      "intent": "Join ticket records to the SLA lookup by matching on Department.",
      "ref": "analysis_df = pd.merge(analysis_df, sla, on='Department')"
    },
    {
      "id": "10_O1_department_rank_starts_at_1",
      "kind": "Operation boundary",
      "node_id": "rank_dept",
      "op": "project",
      "intent": "Assign sequential department ranks starting at 1 after sorting by Achieved % and Department.",
      "ref": "dept_sla['Department Rank'] = range(1, len(dept_sla) + 1)"
    },
    {
      "id": "11_O1_round_achieved_percent_2dp",
      "kind": "Operation boundary",
      "node_id": "rank_dept",
      "op": "project",
      "intent": "Round Achieved % to 2 decimal places in the department ranking output.",
      "ref": "output_03['Achieved %'] = output_03['Achieved %'].round(2)"
    },
    {
      "id": "12_C1_restrict_ranking_to_sla_18_departments",
      "kind": "Row-level concept",
      "node_id": "sla_18_only",
      "op": "filter",
      "intent": "Restrict the department SLA ranking to departments whose SLA Agreement equals 18.",
      "ref": "sla_18_depts = sla[sla['SLA Agreement'] == 18]['Department'].tolist()\nclosed_analysis_filtered = closed_analysis[closed_analysis['Department'].isin(sla_18_depts)]"
    },
    {
      "id": "13_O3_tie_break_by_department_name",
      "kind": "Operation inconsistent",
      "node_id": "sort_dept",
      "op": "sort",
      "source_text": "Rank from highest % to lowest %",
      "ref": "dept_sla = dept_sla.sort_values(by=['Achieved %', 'Department'], ascending=[False, True])"
    },
    {
      "id": "14_C2_start_date_is_earliest_event_timestamp",
      "kind": "Group-level concept",
      "node_id": "start_dates",
      "op": "aggregate",
      "source_text": "how long have they been open, and how many business days has that been?",
      "ref": "start_dates = tickets.groupby('Ticket ID')['Timestamp'].min().reset_index()"
    },
    {
      "id": "15_D1_union_ticket_events_across_multiple_files",
      "kind": "Multi-table alignment",
      "node_id": "tickets_union",
      "op": "union",
      "intent": "Combine ticket status-change events from input_02.csv, input_03.csv, and input_04.csv into a single event table before analysis.",
      "ref": "for fname in ['input_02.csv', 'input_03.csv', 'input_04.csv']:\n    fpath = inputs_dir / fname\n    if fpath.exists():\n        tickets_list.append(pd.read_csv(fpath))\ntickets = pd.concat(tickets_list, ignore_index=True)"
    }
  ]
}